+++
date = '2025-12-21T14:49:14+08:00'
draft = true
description = '强化学习及PPO算法简介'
title = 'Reinforcement Learning'
categories = ['Reinforcement Learning', 'Machine Learning']
+++
## 什么是强化学习
强化学习（Reinforcement Learning，RL）是一种机器学习范式，旨在训练智能体通过与环境交互来学习最佳行为策略，以最大化累积奖励。与监督学习不同，强化学习没有明确的输入输出对，而是通过试错和奖励信号来指导学习过程。
## 强化学习的基本概念
- **智能体（Agent）**：执行动作以与环境交互的实体。
- **环境（Environment）**：智能体所处的外部系统，智能体通过与环境交互来获取状态和奖励。
- **状态（State）**：环境在某一时刻的描述，智能体根据状态做出决策。
- **动作（Action）**：智能体在特定状态下可以执行的操作。
- **奖励（Reward）**：智能体执行动作后从环境中获得的反馈信号，用于评估动作的好坏。
- **策略（Policy）**：智能体在给定状态下选择动作的规则或函数。
- **价值函数（Value Function）**：评估在某一状态下，智能体未来可能获得的累积奖励。
## 马尔可夫决策过程
强化学习通常建模为马尔可夫决策过程（Markov Decision Process，MDP），包括状态空间、动作空间、转移概率和奖励函数。MDP假设未来状态仅依赖于当前状态和动作，而与过去状态无关。
## PPO算法简介
Proximal Policy Optimization（PPO）是一种常用的强化学习算法，属于策略梯度方法。PPO通过限制策略更新的幅度，确保新策略不会偏离旧策略过远，从而提高训练的稳定性和效率。PPO通常使用剪切概率比（clipped probability ratio）来限制策略更新。
### 基本架构
1. **策略网络（Policy Network）**：用于输出在给定状态下选择各个动作的概率分布。
2. **价值网络（Value Network）**：用于估计在给定状态下的价值函数。
### 训练过程
1. **采样数据**：智能体与环境交互，策略网络根据当前状态 s 和自身策略，选择并执行动作 a，收集状态、动作、奖励和下一个状态的数据。
2. **计算优势函数**：使用价值网络估计状态的价值，并计算优势函数以评估动作的好坏。优势用于衡量在给定状态下选择某个动作相对于平均水平的好处，其计算方法通常是用动作价值函数减去状态价值函数。公式如下：  
    <center>A(s, a) = Q(s, a) - V(s)</center> 
    其中， A(s, a)是优势函数，Q(s, a)表示在状态 s 下采取动作 a 随后继续遵循策略π的价值，V(s) 表示在状态 s 下遵循策略π的价值。
3. **更新策略网络**：利用优势函数来更新自己的策略网络，使自己更倾向于选择能获得高评分的动作。同时使用剪切概率比限制策略更新的幅度，确保新策略不会偏离旧策略过远。
4. **更新价值网络**：使用TD误差优化价值网络参数，TD误差可以衡量价值网络预测值与实际回报之间的差异。
